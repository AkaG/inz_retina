{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from text_processing.NGramCounter import NGramCounter\n",
    "from text_processing.NGramVectorBuilder import NGramVectorBuilder\n",
    "from data_module.models import Person, Examination, Description, ImageSeries\n",
    "from data_module.models import Image as Retina_Image\n",
    "from random import shuffle\n",
    "import random, re, PIL, csv, cv2, os, time, sys\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "np.random.seed(7)\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_preparer():\n",
    "    def __init__(self):\n",
    "        self.load_image_resolution()\n",
    "    \n",
    "    def samples_from_file(self, f):\n",
    "            reader = csv.reader(f.readline().split('\\n'), delimiter=',')\n",
    "            for row in reader:\n",
    "                train_samples = np.array(row[:-1],dtype='uint8')\n",
    "                break\n",
    "\n",
    "            reader = csv.reader(f.readline().split('\\n'), delimiter=',')\n",
    "            for row in reader:\n",
    "                val_samples = np.array(row[:-1],dtype='uint8')\n",
    "                break\n",
    "\n",
    "            reader = csv.reader(f.readline().split('\\n'), delimiter=',')\n",
    "            for row in reader:\n",
    "                test_samples = np.array(row[:-1],dtype='uint8')\n",
    "                break\n",
    "            f.close()\n",
    "            return train_samples,val_samples,test_samples\n",
    "    \n",
    "    def load_image_resolution(self):\n",
    "        width = 1388\n",
    "        height = 1038\n",
    "        \n",
    "        self.img_size_2 = 100\n",
    "        self.img_size_1 = int(self.img_size_2 * (height / width))\n",
    "\n",
    "    def get_images_metadata(self, examinations, do_shuffle = True, file=None):\n",
    "        data = []\n",
    "        for examin in examinations:\n",
    "            sequences = ImageSeries.objects.filter(examination=examin)\n",
    "            for i in range(len(sequences)):\n",
    "                if sequences[i].name.endswith(\"after_registration\"):\n",
    "                    continue\n",
    "                if sequences[i].name.startswith(\"left\"):\n",
    "                        y_train = [1,0]\n",
    "                        y_train_inv = [0,1]\n",
    "                else:\n",
    "                        y_train = [0,1]\n",
    "                        y_train_inv = [1,0]\n",
    "                imgModels = Retina_Image.objects.filter(image_series=sequences[i])\n",
    "                for j in range(len(imgModels)):\n",
    "                    data.append({'series': sequences[i].id, 'image_id': imgModels[j].id, 'y_train': y_train, 'image_name': imgModels[j].name, 'invert': False})\n",
    "                    data.append({'series': sequences[i].id, 'image_id': imgModels[j].id, 'y_train': y_train_inv, 'image_name': imgModels[j].name, 'invert': True})\n",
    "        shuffle(data)\n",
    "        return data\n",
    "                            \n",
    "\n",
    "    def prepare_image(self,_id, invert):\n",
    "        img = Retina_Image.objects.get(id=_id)\n",
    "        img = PIL.Image.open(img.image).convert('L')\n",
    "        if invert:\n",
    "            img = img.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n",
    "        arr_img = self.preprocess_image(img)\n",
    "        return arr_img \n",
    "\n",
    "    def standardization(self,image):\n",
    "        return (image - np.mean(image)) / np.std(image)\n",
    "    \n",
    "    def normalize(self,image):\n",
    "        min_val = np.amin(image)\n",
    "        max_val = np.amax(image)\n",
    "\n",
    "        image = image - np.amin(image)\n",
    "        image = image / (max_val - min_val)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def preprocess_image(self,image):\n",
    "        image = np.array(image)\n",
    "        image = self.standardization(image)\n",
    "        image = resize(image, (self.img_size_1, self.img_size_2, 1))\n",
    "        image = self.normalize(image)\n",
    "        return image\n",
    "\n",
    "    def create_dataset_and_store(self,hdf5_file, name, metadata):\n",
    "        x_name = name+'_x'\n",
    "        x_shape = (len(metadata), self.img_size_1, self.img_size_2, 1)\n",
    "        hdf5_file.create_dataset(x_name, x_shape, np.float32)\n",
    "        \n",
    "        y_name = name+'_y'\n",
    "        y_shape = (len(metadata),2)\n",
    "        hdf5_file.create_dataset(y_name, y_shape, np.int8)\n",
    "        \n",
    "        meta_name = name+'_metadata'\n",
    "        meta_shape = (len(metadata),)\n",
    "        hdf5_file.create_group(meta_name)\n",
    "        for i in range(len(metadata)):\n",
    "            meta_elem = metadata[i]\n",
    "\n",
    "            #save X data\n",
    "            prepared_img = self.get_image(meta_elem)\n",
    "            hdf5_file[x_name][i] = prepared_img\n",
    "\n",
    "            #save Y data\n",
    "            hdf5_file[y_name][i] = meta_elem['y_train']\n",
    "            \n",
    "            #save metadata\n",
    "            gr = hdf5_file.create_group(meta_name+'/'+str(i))\n",
    "            for k, v in meta_elem.items():\n",
    "                    gr[k] = v\n",
    "                    \n",
    "                    \n",
    "    def get_image(self,meta_elem):\n",
    "        id_img = meta_elem['image_id']\n",
    "        invert = meta_elem['invert']\n",
    "        return self.prepare_image(id_img, invert)\n",
    "    \n",
    "                \n",
    "    def store_all_data_in_h5py_file(self):\n",
    "        \n",
    "        f = open('splited_data.txt')\n",
    "        train_samples, val_samples, test_samples = self.samples_from_file(f)\n",
    "\n",
    "        train_metadata = self.get_images_metadata(train_samples)\n",
    "        val_metadata = self.get_images_metadata(val_samples)\n",
    "        test_metadata = self.get_images_metadata(test_samples)\n",
    "\n",
    "        hdf5_path = './lr-data-size100.hdf5'\n",
    "        hdf5_file = h5py.File(hdf5_path, mode='w')\n",
    "        \n",
    "        try: \n",
    "            self.create_dataset_and_store(hdf5_file,'train_data',train_metadata)\n",
    "            self.create_dataset_and_store(hdf5_file,'val_data',val_metadata)\n",
    "            self.create_dataset_and_store(hdf5_file,'test_data',test_metadata)\n",
    "            hdf5_file.close()\n",
    "            print('success')\n",
    "\n",
    "        except:\n",
    "            print('fail')\n",
    "            hdf5_file.close()\n",
    "            os.remove(hdf5_path)\n",
    "            raise\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparer = Data_preparer()\n",
    "data_preparer.store_all_data_in_h5py_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
