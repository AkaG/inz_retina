{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from neural_network.nn_manager.TrainManager import TrainManager\n",
    "from neural_network.store.DBNNSave import DBNNSave\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Reshape, Input, Conv2D, BatchNormalization\n",
    "from keras.layers.convolutional import Convolution1D, Convolution2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras.backend as K\n",
    "import keras\n",
    "from random import shuffle\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDetectionNN(TrainManager):\n",
    "    def __init__(self):\n",
    "        self.path_to_data = './sd-size-150.hdf5'\n",
    "        self.prepare_data(self.path_to_data)\n",
    "        self.batch_size = 64\n",
    "        self.epochs = 20\n",
    "        super(SequenceDetectionNN, self).__init__()\n",
    "        \n",
    "        self.datagen_train = ImageDataGenerator(\n",
    "            vertical_flip=True,\n",
    "            horizontal_flip=True,\n",
    "            zoom_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            rotation_range = 45\n",
    "        )\n",
    "        \n",
    "        self.datagen_val = ImageDataGenerator(\n",
    "        )\n",
    "        \n",
    "    def prepare_data(self, path):\n",
    "        hdf5_file = h5py.File(path, 'r')\n",
    "        self.get_handlers(hdf5_file)\n",
    "        self.load_sizes()\n",
    "        \n",
    "    def load_sizes(self):\n",
    "        self.img_size_1 = self.X_train.shape[1]\n",
    "        self.img_size_2 = self.X_train.shape[2]\n",
    "        self.outputs_size = self.Y_train.shape[1]\n",
    "        self.num_train_samples = self.X_train.shape[0]\n",
    "        self.num_val_samples = self.X_val.shape[0]\n",
    "    \n",
    "    def get_handlers(self, file):\n",
    "        self.X_train = file['train_data_x']\n",
    "        self.Y_train = file['train_data_y']\n",
    "        self.X_val = file['val_data_x']\n",
    "        self.Y_val = file['val_data_y']\n",
    "    \n",
    "    def store_method(self):\n",
    "        return DBNNSave()\n",
    "\n",
    "    def train_data_generator(self):\n",
    "        generator = self._generator(self.X_train, self.Y_train, self.datagen_train)\n",
    "        return generator\n",
    "\n",
    "    def test_data_generator(self):\n",
    "        generator = self._generator(self.X_val, self.Y_val, self.datagen_val)\n",
    "        return generator\n",
    "        \n",
    "    def store_method(self):\n",
    "        return DBNNSave()\n",
    "\n",
    "    def create_model(self):\n",
    "        input_image_1 = Input(shape=(self.img_size_1, self.img_size_2, 1))\n",
    "        input_image_2 = Input(shape=(self.img_size_1, self.img_size_2, 1))\n",
    "        merged_vector = keras.layers.concatenate([input_image_1, input_image_2], axis=-1)\n",
    "        \n",
    "        layer = Conv2D(filters=32, kernel_size=(3, 3))(merged_vector)\n",
    "        layer = BatchNormalization(axis=1)(layer)\n",
    "        layer = Activation('relu')(layer)\n",
    "        layer = MaxPooling2D(pool_size=(2, 2))(layer)\n",
    "        \n",
    "        layer = Conv2D(filters=32, kernel_size=(3, 3))(layer)\n",
    "        layer = BatchNormalization(axis=1)(layer)\n",
    "        layer = Activation('relu')(layer)\n",
    "        layer = MaxPooling2D(pool_size=(2, 2))(layer)\n",
    "        \n",
    "        layer = Conv2D(filters=32, kernel_size=(3, 3))(layer)\n",
    "        layer = BatchNormalization(axis=1)(layer)\n",
    "        layer = Activation('relu')(layer)\n",
    "        layer = MaxPooling2D(pool_size=(2, 2))(layer)\n",
    "        \n",
    "        layer = Flatten()(layer)\n",
    "        \n",
    "        layer = Dense(1024)(layer)\n",
    "        layer = BatchNormalization(axis=1)(layer)\n",
    "        layer = Activation('relu')(layer)\n",
    "        \n",
    "        layer = Dense(32)(layer)\n",
    "        layer = BatchNormalization(axis=1)(layer)\n",
    "        layer = Activation('relu')(layer)\n",
    "\n",
    "        layer = Dense(1)(layer)\n",
    "        layer = BatchNormalization(axis=1)(layer)\n",
    "        output_layer = Activation('sigmoid')(layer)\n",
    "        model = Model(inputs=[input_image_1, input_image_2], outputs=output_layer)\n",
    "        model.compile( optimizer='adam',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def _generator(self,X,Y,datagen):\n",
    "        X_1 = np.expand_dims(X[:,:,:,0],3); X_2 = np.expand_dims(X[:,:,:,1],3)\n",
    "        batches_1 = datagen.flow(X_1,Y, batch_size=self.batch_size, shuffle = True, seed = 7)\n",
    "        batches_2 = datagen.flow(X_2,Y, batch_size=self.batch_size, shuffle = True, seed = 7)\n",
    "\n",
    "        while 1:\n",
    "             for batch_1, batch_2 in zip(batches_1,batches_2):\n",
    "                x1 = batch_1[0]\n",
    "                x2 = batch_2[0]\n",
    "                y = batch_1[1]\n",
    "                yield [x1,x2], y\n",
    "\n",
    "        \n",
    "    def train(self):\n",
    "        self.train_model(\n",
    "            self.num_train_samples // self.batch_size,\n",
    "            self.num_val_samples // self.batch_size,\n",
    "            epochs=self.epochs\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sd = SequenceDetectionNN()\n",
    "history = sd.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network.store.ModelLoader import load_weights_from_file\n",
    "sd = SequenceDetectionNN()\n",
    "nn = NeuralNetwork.objects.get(id=161)\n",
    "sd.model = load_weights_from_file(model=sd.model, file_path=nn.weights.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "import re\n",
    "    \n",
    "def get_tau(struct):\n",
    "    sorted_struct = sorted(struct.items(), key=lambda x:x[1])\n",
    "    order_predicted = []; order_original = []; i = 0\n",
    "    for item in sorted_struct:\n",
    "        key,value = item\n",
    "        order_predicted.append(key)\n",
    "        order_original.append(i)\n",
    "        i = i + 1\n",
    "    #print(order_predicted)\n",
    "    tau, p_value = kendalltau(order_original, order_predicted)\n",
    "    return tau\n",
    "    \n",
    "def predict_data(model, x_test):\n",
    "    x_1 = np.expand_dims(x_test[:,:,:,0],3); x_2 = np.expand_dims(x_test[:,:,:,1],3)\n",
    "    return model.predict([x_1,x_2])\n",
    "\n",
    "def get_name(data):\n",
    "    img_name = data.value\n",
    "    return int(re.search(r'\\d+', img_name).group())\n",
    "    \n",
    "def add_result(meta, seq_res, y_score):\n",
    "    first = get_name(meta['first_name']); second = get_name(meta['second_name'])\n",
    "    if first not in seq_res:\n",
    "        seq_res[first] = 0\n",
    "    if second not in seq_res:\n",
    "        seq_res[second] = 0\n",
    "    seq_res[first] += y_score\n",
    "    seq_res[second] += (1-y_score)\n",
    "\n",
    "def calc_tau(Meta, y_score):\n",
    "    results = dict()\n",
    "    for i in range(len(Meta)):\n",
    "        meta = Meta[str(i)]\n",
    "        series_id = meta['series'].value\n",
    "        if series_id not in results:\n",
    "            results[series_id] = dict()\n",
    "        add_result(meta,results[series_id], y_score[i])\n",
    "    \n",
    "    mean_tau = []\n",
    "    i = 0\n",
    "    for item in results.items():\n",
    "        k, v = item\n",
    "        tau = get_tau(v)\n",
    "        mean_tau.append(tau)\n",
    "        print(k,tau)\n",
    "        i = i + 1\n",
    "    print(len(mean_tau))\n",
    "    print(np.mean(mean_tau))\n",
    "    \n",
    "    \n",
    "def test(nn):\n",
    "    file = h5py.File(nn.path_to_data, 'r')\n",
    "    X_test = file['test_data_x']; Y_test = file['test_data_y']; Meta = file['test_data_metadata']\n",
    "    print(len(Meta))\n",
    "    y_score = predict_data(nn.model,X_test)\n",
    "    calc_tau(Meta, y_score)\n",
    "    \n",
    "    \n",
    "test(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File(sd.path_to_data, 'r')\n",
    "X_test = file['test_data_x']; Y_test = file['test_data_y']; Meta = file['test_data_metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
